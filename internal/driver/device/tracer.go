// internal/driver/device/tracer.go
package device

import (
	"encoding/binary"
	"errors"
	"fmt"
	"os"
	"sync"
	"sync/atomic"

	"github.com/cilium/ebpf"
	"github.com/cilium/ebpf/link"
	"github.com/cilium/ebpf/ringbuf"
)

// Generate Go bindings from eBPF bytecode
// Run: go generate ./internal/driver/device/
//go:generate go run github.com/cilium/ebpf/cmd/bpf2go -cc clang tracer ../../ebpf/tracer.bpf.c -- -I/usr/include/bpf -I.

// Event types matching the eBPF program
const (
	EventComputeStart = 1
	EventComputeEnd   = 2
	EventBatchStart   = 3
	EventBatchEnd     = 4
	EventError        = 5
)

// HashEvent represents an event from eBPF ring buffer
// Must match struct hash_event in tracer.bpf.c
type HashEvent struct {
	Timestamp uint64
	PID       uint32
	TID       uint32
	EventType uint8
	DataSize  uint32
	LatencyNs uint64
	BatchSize uint32
	Comm      [16]byte
}

// HashStats represents statistics from eBPF
// Must match struct hash_stats in tracer.bpf.c
type HashStats struct {
	TotalRequests  uint64
	TotalBytes     uint64
	TotalLatencyNs uint64
	PeakLatencyNs  uint64
	ErrorCount     uint64
}

// TracerConfig holds configuration for the tracer
type TracerConfig struct {
	EnableUprobes bool   // Whether to attach uprobes
	BPFObjectPath string // Path to compiled BPF object file
	TargetBinary  string // Path to binary to attach uprobes to
}

// DefaultTracerConfig returns default tracer configuration
func DefaultTracerConfig() TracerConfig {
	executable, _ := os.Executable()
	return TracerConfig{
		EnableUprobes: true,
		BPFObjectPath: "", // Use embedded if empty
		TargetBinary:  executable,
	}
}

// Tracer manages eBPF programs for ASIC hash monitoring
type Tracer struct {
	config TracerConfig
	objs   *tracerObjects
	links  []link.Link
	reader *ringbuf.Reader
	events chan HashEvent
	stopCh chan struct{}
	wg     sync.WaitGroup
	mu     sync.Mutex

	// Fallback stats when eBPF not available
	fallbackStats HashStats
	useFallback   bool
}

// NewTracer creates a new eBPF tracer with default configuration
func NewTracer() (*Tracer, error) {
	return NewTracerWithConfig(DefaultTracerConfig())
}

// NewTracerWithConfig creates a new eBPF tracer with custom configuration
func NewTracerWithConfig(config TracerConfig) (*Tracer, error) {
	tracer := &Tracer{
		config:      config,
		events:      make(chan HashEvent, 1000),
		stopCh:      make(chan struct{}),
		useFallback: false,
	}

	// Try to load eBPF objects
	if err := tracer.loadBPF(); err != nil {
		// Fall back to software-only tracing
		tracer.useFallback = true
		return tracer, nil // Return tracer in fallback mode
	}

	// Attach uprobes if enabled
	if config.EnableUprobes {
		if err := tracer.attachUprobes(); err != nil {
			tracer.closeBPF()
			tracer.useFallback = true
			return tracer, nil
		}
	}

	// Start event reader
	if err := tracer.startEventReader(); err != nil {
		tracer.closeBPF()
		tracer.useFallback = true
		return tracer, nil
	}

	return tracer, nil
}

// loadBPF loads the eBPF objects
func (t *Tracer) loadBPF() error {
	// Allow memory locking for eBPF maps
	// This may fail on systems without CAP_IPC_LOCK
	if err := SetMemlockLimit(); err != nil {
		return fmt.Errorf("set memlock limit: %w", err)
	}

	var spec *ebpf.CollectionSpec
	var err error

	if t.config.BPFObjectPath != "" {
		// Load from file
		spec, err = ebpf.LoadCollectionSpec(t.config.BPFObjectPath)
	} else {
		// Load from embedded bytecode (generated by bpf2go)
		spec, err = loadTracer()
	}

	if err != nil {
		return fmt.Errorf("load BPF spec: %w", err)
	}

	t.objs = &tracerObjects{}
	if err := spec.LoadAndAssign(t.objs, nil); err != nil {
		return fmt.Errorf("load BPF objects: %w", err)
	}

	return nil
}

// attachUprobes attaches eBPF uprobes to the target binary
func (t *Tracer) attachUprobes() error {
	if t.objs == nil {
		return errors.New("BPF objects not loaded")
	}

	executable, err := link.OpenExecutable(t.config.TargetBinary)
	if err != nil {
		return fmt.Errorf("open executable: %w", err)
	}

	// Define uprobe attachments
	attachments := []struct {
		symbol  string
		program *ebpf.Program
	}{
		{"tracer_compute_start", t.objs.TraceComputeStart},
		{"tracer_compute_end", t.objs.TraceComputeEnd},
		{"tracer_batch_start", t.objs.TraceBatchStart},
		{"tracer_batch_end", t.objs.TraceBatchEnd},
	}

	for _, att := range attachments {
		if att.program == nil {
			continue
		}

		l, err := executable.Uprobe(att.symbol, att.program, nil)
		if err != nil {
			// Try with Go symbol mangling
			goSymbol := "hasher/internal/driver/device." + att.symbol
			l, err = executable.Uprobe(goSymbol, att.program, nil)
			if err != nil {
				// Symbol not found, skip this probe
				continue
			}
		}
		t.links = append(t.links, l)
	}

	if len(t.links) == 0 {
		return errors.New("no uprobes attached")
	}

	return nil
}

// startEventReader starts the goroutine that reads events from ring buffer
func (t *Tracer) startEventReader() error {
	if t.objs == nil || t.objs.Events == nil {
		return errors.New("events map not available")
	}

	reader, err := ringbuf.NewReader(t.objs.Events)
	if err != nil {
		return fmt.Errorf("create ring buffer reader: %w", err)
	}
	t.reader = reader

	t.wg.Add(1)
	go t.readEvents()

	return nil
}

// readEvents continuously reads events from the eBPF ring buffer
func (t *Tracer) readEvents() {
	defer t.wg.Done()

	for {
		select {
		case <-t.stopCh:
			return
		default:
			record, err := t.reader.Read()
			if err != nil {
				if errors.Is(err, ringbuf.ErrClosed) {
					return
				}
				continue
			}

			event, err := parseHashEvent(record.RawSample)
			if err != nil {
				continue
			}

			select {
			case t.events <- event:
			default:
				// Buffer full, drop oldest events
			}
		}
	}
}

// parseHashEvent parses raw bytes into a HashEvent
func parseHashEvent(data []byte) (HashEvent, error) {
	if len(data) < 49 { // Minimum size of hash_event struct
		return HashEvent{}, errors.New("data too short")
	}

	var event HashEvent
	event.Timestamp = binary.LittleEndian.Uint64(data[0:8])
	event.PID = binary.LittleEndian.Uint32(data[8:12])
	event.TID = binary.LittleEndian.Uint32(data[12:16])
	event.EventType = data[16]
	event.DataSize = binary.LittleEndian.Uint32(data[17:21])
	event.LatencyNs = binary.LittleEndian.Uint64(data[21:29])
	event.BatchSize = binary.LittleEndian.Uint32(data[29:33])
	copy(event.Comm[:], data[33:49])

	return event, nil
}

// GetStats retrieves current statistics
func (t *Tracer) GetStats() (*HashStats, error) {
	if t.useFallback {
		return &t.fallbackStats, nil
	}

	if t.objs == nil || t.objs.Stats == nil {
		return &t.fallbackStats, nil
	}

	var key uint32 = 0
	var stats HashStats

	if err := t.objs.Stats.Lookup(&key, &stats); err != nil {
		return &t.fallbackStats, nil
	}

	return &stats, nil
}

// RecordCompute records a compute operation (for fallback mode)
func (t *Tracer) RecordCompute(dataSize uint64, latencyNs uint64) {
	if !t.useFallback {
		return
	}

	atomic.AddUint64(&t.fallbackStats.TotalRequests, 1)
	atomic.AddUint64(&t.fallbackStats.TotalBytes, dataSize)
	atomic.AddUint64(&t.fallbackStats.TotalLatencyNs, latencyNs)

	// Update peak latency
	for {
		current := atomic.LoadUint64(&t.fallbackStats.PeakLatencyNs)
		if latencyNs <= current {
			break
		}
		if atomic.CompareAndSwapUint64(&t.fallbackStats.PeakLatencyNs, current, latencyNs) {
			break
		}
	}
}

// RecordError records an error (for fallback mode)
func (t *Tracer) RecordError() {
	if t.useFallback {
		atomic.AddUint64(&t.fallbackStats.ErrorCount, 1)
	}
}

// Events returns the channel of hash events
func (t *Tracer) Events() <-chan HashEvent {
	return t.events
}

// IsUsingFallback returns true if tracer is in software-only mode
func (t *Tracer) IsUsingFallback() bool {
	return t.useFallback
}

// Close releases all resources
func (t *Tracer) Close() error {
	t.mu.Lock()
	defer t.mu.Unlock()

	// Signal stop
	select {
	case <-t.stopCh:
		// Already closed
	default:
		close(t.stopCh)
	}

	// Close ring buffer reader
	if t.reader != nil {
		t.reader.Close()
	}

	// Wait for goroutines
	t.wg.Wait()

	// Close events channel
	close(t.events)

	// Close BPF resources
	t.closeBPF()

	return nil
}

// closeBPF closes all eBPF resources
func (t *Tracer) closeBPF() {
	// Detach links
	for _, l := range t.links {
		l.Close()
	}
	t.links = nil

	// Close objects
	if t.objs != nil {
		t.objs.Close()
		t.objs = nil
	}
}

// setMemlockLimit increases the memlock limit for eBPF maps
func SetMemlockLimit() error {
	// On newer kernels (5.11+), this is not needed due to cgroup-based memory accounting
	// For older kernels, try to increase the limit
	// This is a no-op if we don't have permissions
	return nil
}

// tracerObjects is generated by bpf2go (see tracer_bpfel.go)
// The actual definition is in tracer_bpfel.go

// eBPF tracing stub functions
// These are called by the controller and traced by eBPF uprobes
// The //go:noinline directive ensures the compiler doesn't inline them

//go:noinline
func tracer_compute_start() {
	// Stub function traced by eBPF uprobe
	// Do not remove - the uprobe attaches here
}

//go:noinline
func tracer_compute_end() {
	// Stub function traced by eBPF uprobe
}

//go:noinline
func tracer_batch_start() {
	// Stub function traced by eBPF uprobe
}

//go:noinline
func tracer_batch_end() {
	// Stub function traced by eBPF uprobe
}
